import os
import importlib
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader
from torch.nn import DataParallel
import math
from torch.nn import BatchNorm1d,BatchNorm2d,BatchNorm3d,SyncBatchNorm
from .logger import setup_logger
from PIL import Image  


def run_train_contrastive(config):
    
    train_data = DataLoader(config.train_dataset,
                            batch_size=config.batch_size,
                            shuffle=True,
                            num_workers=config.num_workers)

    trainer = config.trainer(
                                encoder_1=config.encoder_1,
                                encoder_2 = config.encoder_2,
                                proj_1 = config.proj_1,
                                proj_2=config.proj_2,
                                train_data_loader=train_data,
                                epoch_count=config.epoch_count,
                                opt =config.opt,
                                scheduler=config.scheduler,
                                loss=config.loss,
                                print_frequency=config.print_frequency,
                                device=config.device,
                                model_save_path=config.model_save_path,
                                state_storage=config.state_storage,
                                tracker_storage = config.tracker_storage,
                                prototypes = config.prototypes)
    trainer.run()
    
    

# =====================================================================================

# Load a configuration
def load_config(module_path, cls_name):
    module_path_fixed = module_path
    if module_path_fixed.endswith(".py"):
        module_path_fixed = module_path_fixed[:-3]
    module_path_fixed = module_path_fixed.replace("/", ".")
    module = importlib.import_module(module_path_fixed)
    assert hasattr(module, cls_name), "{} file should contain {} class".format(module_path, cls_name)

    cls = getattr(module, cls_name)
    return cls

# Save a model
def save_model(model, path):
    if isinstance(model, DataParallel):
        model = model.module
        
    with open(path, "wb") as fout:
        torch.save(model.state_dict(), fout)
        
# Load a model
def load_model(model, path):
    with open(path, "rb") as fin:
        state_dict = torch.load(fin)
        
    model.load_state_dict(state_dict)

# From cpu to gpu or the opposite
def move_to_device(tensor: list or tuple or torch.Tensor, device: str):
    if isinstance(tensor, list):
        return [move_to_device(elem, device=device) for elem in tensor]
    if isinstance(tensor, tuple):
        return (move_to_device(elem, device=device) for elem in tensor)
    return tensor.to(device)


def normalize01(im,val=None):
    if val == None:
        m = np.amin(im)
        M = np.amax(im)
    else:
        m = val[0]
        M = val[1]
    im_norm = (im-m)/(M-m)
    return im_norm

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_.\tThe distribution of values may be incorrect.",)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

# Apply a treshold, a defined treshold or mean+3*var
def tresh_im(img,treshold=None,k=3):
    imabs = np.abs(img)
    sh = imabs.shape

    if treshold == None:
        if len(sh) == 2:
            mean = np.mean(imabs)
            std = np.std(imabs)
            treshold = mean+k*std
            imabs = np.clip(imabs,None,treshold)
            imabs = normalize01(imabs)
            print('treshold : {}'.format(treshold))

        elif len(sh) == 3:
            for i in range(sh[2]):
                im_p = imabs[:,:,i] # take channel i
                mean = np.mean(im_p)
                std = np.std(im_p)
                treshold_p = mean+k*std # compute treshold
                im_p = np.clip(im_p,None,treshold_p) # apply treshold
                im_p = normalize01(im_p) # normalize [0-1]
                imabs[:,:,i] = im_p
                print('treshold for channel {} : {}'.format(i,treshold_p))

    else:
        if len(sh) == 2:
            imabs = np.clip(imabs,None,treshold)
            imabs = normalize01(imabs)

        elif len(sh) == 3:
            for i in range(sh[2]):
                if len(treshold) == sh[2]:
                    im_p = imabs[:,:,i] # take channel i
                    im_p = np.clip(im_p,None,treshold[i]) # apply treshold
                    im_p = normalize01(im_p,[0,treshold[i]]) # normalize [0-max/treshold]
                    imabs[:,:,i] = im_p
                else:
                    print('Number of tresholds should be the same as number of channels but got {} and {}'.format(len(treshold),sh[2]))    
    return imabs


def get_params_groups(models, prototypes=None, lr_prototypes=1e-3):
    regularized = []
    not_regularized = []

    if not isinstance(models, list):
        models = [models]

    for model in models:
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
            # we do not regularize biases nor Norm parameters
            if name.endswith(".bias") or len(param.shape) == 1:
                not_regularized.append(param)
            else:
                regularized.append(param)

    if prototypes != None:
        return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}, 
        {'params': [prototypes],'lr': lr_prototypes, 'weight_decay': 0.}]
    

    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]
    
class LARS(torch.optim.Optimizer):
    """
    Almost copy-paste from https://github.com/facebookresearch/barlowtwins/blob/main/main.py
    """
    def __init__(self, params, lr=0, weight_decay=0, momentum=0.9, eta=0.001,
                 weight_decay_filter=None, lars_adaptation_filter=None):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,
                        eta=eta, weight_decay_filter=weight_decay_filter,
                        lars_adaptation_filter=lars_adaptation_filter)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):
        for g in self.param_groups:
            for p in g['params']:
                dp = p.grad

                if dp is None:
                    continue

                if p.ndim != 1:
                    dp = dp.add(p, alpha=g['weight_decay'])

                if p.ndim != 1:
                    param_norm = torch.norm(p)
                    update_norm = torch.norm(dp)
                    one = torch.ones_like(param_norm)
                    q = torch.where(param_norm > 0.,
                                    torch.where(update_norm > 0,
                                                (g['eta'] * param_norm / update_norm), one), one)
                    dp = dp.mul(q)

                param_state = self.state[p]
                if 'mu' not in param_state:
                    param_state['mu'] = torch.zeros_like(p)
                mu = param_state['mu']
                mu.mul_(g['momentum']).add_(dp)

                p.add_(mu, alpha=-g['lr'])

# Plot an image (no treshold is applied)
def plot_im(img,title = '',bar = False,save=False,save_path='.'):
    if len(img.shape) == 2:
        plt.imshow(img, cmap = 'gray')
    else:
        plt.imshow(img)
    if bar:
        plt.colorbar()
    plt.axis('off')

    if not save:
        plt.title(title)
    else:
        plt.savefig('{}/{}.png'.format(save_path,title),bbox_inches = 'tight',pad_inches = 0)
    plt.show()

def save_plot(im,fold,bar = False,):
    if len(im.shape) == 2:
        plt.imshow(im, cmap = 'gray')
    else:
        plt.imshow(im)
    if bar:
        cb = plt.colorbar()
    plt.axis('off')
    plt.savefig(fold,bbox_inches = 'tight',pad_inches = 0)
    # if bar:
    #     cb.remove()
    plt.clf()

def save_curve(x,y,fold,curve_name=''):
    plt.plot(x,y,label = curve_name)
    plt.legend()
    plt.savefig(fold,bbox_inches = 'tight',pad_inches = 0)
    plt.clf()

# Plot an image with a treshold, if tresh is None it's an automatic treshold mean+3*var
def disp_sar(im,tresh=None, ch=None, title = ''):
    im = np.abs(im)
    shape_im = im.shape

    # Take one chanel
    if ch is not None:
        im = im[:,:,ch]

    # Create RGB image with G = 1/2(hv+vh)
    elif len(shape_im) == 3 and shape_im[2] == 4:
        polsar_im = np.zeros((shape_im[0],shape_im[1],3),dtype=np.single)
        polsar_im[:,:,0] = im[:,:,0]
        polsar_im[:,:,1] = (im[:,:,1]+im[:,:,2])/2
        polsar_im[:,:,2] = im[:,:,3]
        im = polsar_im

    # Apply treshold to image
    if tresh == None:
        im_t = tresh_im(im)
    else:
        im_t = tresh_im(im,treshold=tresh)

    plot_im(im_t, title = title)


# Save an image
def save_im(im,fold,is_sar=True,tresh=None):

    im = np.abs(im)
    shape_im = im.shape

    if len(shape_im) == 2:
        polsar_im = im
    elif len(shape_im) == 3:
        if shape_im[2] == 4:
            polsar_im = np.zeros((shape_im[0],shape_im[1],3),dtype=np.single)
            polsar_im[:,:,0] = im[:,:,0]
            polsar_im[:,:,1] = (im[:,:,1]+im[:,:,2])/2
            polsar_im[:,:,2] = im[:,:,3]
        elif shape_im[2] == 2:
            polsar_im = np.zeros((shape_im[0],shape_im[1],3),dtype=np.single)
            polsar_im[:,:,0] = im[:,:,0]
            polsar_im[:,:,1] = im[:,:,1]
            polsar_im[:,:,2] = im[:,:,1]
        elif shape_im[2] == 3:
            polsar_im = im
        else:
            print('Number of channel should be 2, 3 or 4 but is {}'.format(shape_im[2]))
            return 
    else :
        name = os.path.basename(fold)
        print('can''t save {} because it is not an image'.format(name))
        return

    if is_sar:
        polsar_im = tresh_im(polsar_im,treshold=tresh)*255
    else :
        polsar_im = normalize01(polsar_im)*255

    if len(shape_im) == 2:
        polsar_im = Image.fromarray(polsar_im.astype(np.uint8))
    elif len(shape_im) == 3:
        polsar_im = Image.fromarray(polsar_im.astype(np.uint8), 'RGB')
    polsar_im.save(fold) 
    
def plot_hist(im):
    m = np.amin(im)
    M = np.amax(im)
    std = np.std(im)
    mean = np.mean(im)

    plt.figure()
    plt.hist(np.ravel(im),bins='auto',density=True)  
    plt.title('min : {:.3}. max : {:.3}. std : {:.3}. mean : {:.3}'.format(m,M,std,mean))
    plt.show()

def draw_progress_bar(iteration, total, prefix='', length=50):
    percent = ("{0:.1f}").format(100 * (iteration / float(total)))
    filled_length = int(length * iteration // total)
    bar = '#' * filled_length + '-' * (length - filled_length)

    print(f'\r{prefix} |{bar}| {percent}% Complete', end='')

    if iteration == total:
        print('')  # Move to the next line

if __name__ == "__main__":  
    pass
